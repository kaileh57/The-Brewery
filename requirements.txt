transformers>=4.36.0
torch>=2.0.0
accelerate>=0.25.0
peft>=0.6.0
datasets>=2.14.0
google-generativeai>=0.3.0
wandb>=0.15.0
numpy>=1.24.0
tqdm>=4.65.0
sentencepiece>=0.1.99
protobuf>=4.23.0
bitsandbytes>=0.41.0  # For 4-bit quantization
flash-attn>=2.3.0     # For flash attention support
